{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80e7e2-ed10-4a39-a46b-4dbbcc505b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a52b9-94e2-44ec-b678-5c3bf7374c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c1c98-4967-41d8-8c32-9bbcc0bc4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef795951-6f8b-4a62-af5e-10d8ab59846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44004bf-34ed-4e4f-9d25-9dc1ec32bfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6cdcfe-abb6-4d2a-80e2-782f6b71144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimilarityResult:\n",
    "    \"\"\"Class to hold detailed similarity analysis results.\"\"\"\n",
    "    category: str\n",
    "    similarity_score: float\n",
    "    matched_content: str\n",
    "    content_type: str  # 'word' or 'sentence'\n",
    "    risk_level: str    # 'high', 'medium', 'low'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QueryAnalysis:\n",
    "    \"\"\"Class to hold comprehensive query analysis.\"\"\"\n",
    "    query: str\n",
    "    is_safe: bool\n",
    "    flagged_categories: List[str]\n",
    "    detailed_matches: List[SimilarityResult]\n",
    "    highest_similarity: float\n",
    "    primary_category: Optional[str]\n",
    "    risk_assessment: str\n",
    "    processing_time: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d731b-d5c1-4568-8c41-e064ba3f527c",
   "metadata": {},
   "source": [
    "Functions defined are -\n",
    "\n",
    "* __init__(self) - This function initializes a matcher object by setting up model placeholders, thresholds for risk categories, safe term whitelists, and then calls a system initialization method.\n",
    "* _load_model(self) - This function tries to load a locally saved sentence transformer model from snapshot directories (all-MiniLM-L6-v2).\n",
    "* _load_embeddings(self) - This function loads topics and sentences from JSON files, encodes them into embeddings using the model, and stores them in dictionaries. ( Scope of improvement - embeddings for the files to be stored and retrieved when needed instead of creating embeddings multiple times )\n",
    "* _setup_categorical_hierarchies(self) - This function defines hierarchical relationships between categories, grouping them under broader parents with assigned severity levels.\n",
    "* _clean_query(self, query) - This function cleans and tokenizes a query into words, removing stopwords, non-alphabetic tokens, and very short words. If tokenization fails, it falls back to a simpler method.\n",
    "* _is_safe_query(self, query) - This function checks whether a query is safe by looking for educational patterns, geographical terms, safe starters, and overlapping safe terms, then returns a boolean decision.\n",
    "* _is_common_word_match(self, matched_content, query) - This function checks if a match between query and content comes only from common safe words, treating it as an innocent match.\n",
    "* _filter_false_positives(self,results,query) - This function filters similarity results to reduce false positives. For safe queries, it applies stricter thresholds and checks before keeping results, while critical categories are treated more leniently.\n",
    "* _calculate_detailed_similarity(self, query, threshold) - This function computes detailed similarity scores between a user’s query and predefined risky topics and sentences. It first performs word-level analysis by tokenizing and embedding individual words from the query, comparing them against stored topic embeddings to find the most similar risky word in each category. Then, it performs sentence-level analysis by embedding the entire query and comparing it with stored sentence embeddings to detect close matches. For each significant match that meets or exceeds a similarity threshold, the function records details such as category, similarity score, matched content, type (word or sentence), and associated risk level. It finally sorts all matches in descending order of similarity and applies a filtering mechanism to reduce false positives, returning a cleaned list of similarity results.\n",
    "* _determine_risk_level(self, similarity_score, category) - This function assigns a risk level to a match based on its similarity score and category, using stricter thresholds for critical categories and standard thresholds otherwise.\n",
    "* analyze_query(self, query) -> QueryAnalysis - This function performs a full analysis of a user query by computing similarity scores against predefined risky words and sentences, identifying categories that may be flagged, and assessing the overall risk. It first calculates detailed similarity results using a balanced threshold, then identifies all categories triggered by the query. High and medium risk matches are used to determine whether the query is considered safe. The function also extracts the highest similarity score and the primary category with the closest match. A risk assessment summary is generated based on the detailed matches and flagged categories. Finally, the function records processing time and returns a structured QueryAnalysis object containing the query, safety status, top categories, top matches, risk assessment, and processing metrics.\n",
    "* _generate_risk_assessment(self, similarity_results, flagged_categories) -> str: This function generates a textual risk assessment based on the similarity results of a query. It first checks if there are any similarity results; if none exist, the query is considered safe. It then separates high-risk and medium-risk matches. If high-risk matches are found, it reports the primary category with the highest similarity score and marks the query as “High Risk.” If only medium-risk matches are present, it reports the primary category and labels it as “Medium Risk.” If no high or medium risks are detected, the function summarizes the situation as “Low Risk” and notes the number of categories with minor concerns.\n",
    "* get_category_breakdown(self, query) - This function provides a comprehensive breakdown of a query by category, summarizing statistics such as matches, maximum and average similarity scores, risk levels, and severity. It first analyzes the query to get detailed similarity matches. For each match, it aggregates data into a dictionary per category, updating the maximum similarity score, computing risk levels based on the highest risk encountered, and retrieving the severity of the category. After processing all matches, it calculates the average similarity per category. Finally, it returns a structured dictionary containing detailed category statistics for further inspection or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1e082-0e07-4a02-80ba-1680cf230f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31d58c-76e4-481f-9faa-8a8aef37a003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AdvancedQueryMatcher:\n",
    "    \"\"\"\n",
    "    Advanced query matching system with comprehensive categorization,\n",
    "    confidence scoring, and detailed similarity analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Initialize the matcher with model and data loading.\n",
    "        \"\"\"\n",
    "        self.model = None  # Placeholder for the model, not loaded yet\n",
    "        self.embeddings = {}  # Dictionary to store embeddings\n",
    "        self.sentence_embeddings = {}  # Dictionary to store sentence embeddings\n",
    "        self.category_hierarchies = {}  # Dictionary to store category hierarchy mappings\n",
    "        \n",
    "        # Define general risk thresholds\n",
    "        self.risk_thresholds = {\n",
    "            'high_risk': 0.75,     # Threshold for high-risk detection\n",
    "            'medium_risk': 0.65,   # Threshold for medium-risk detection\n",
    "            'low_risk': 0.55,      # Threshold for low-risk detection\n",
    "            'safe': 0.45           # Threshold for safe detection\n",
    "        }\n",
    "        \n",
    "        # Define stricter thresholds for critical categories\n",
    "        self.critical_category_thresholds = {\n",
    "            'self_harm': 0.65,                      # Threshold for self-harm\n",
    "            'violence_and_crime': 0.70,            # Threshold for violence/crime\n",
    "            'weapons_and_warfare': 0.70,           # Threshold for weapons/warfare\n",
    "            'financial_and_illegal_activities': 0.70,  # Threshold for financial/illegal\n",
    "            'child_safety': 0.60,                  # Threshold for child safety\n",
    "            'sexual_content': 0.65                 # Threshold for sexual content\n",
    "        }\n",
    "        \n",
    "        # Whitelist of safe terms that should not be flagged - MORE SAFE TERMS COULD BE ADDED HERE\n",
    "        self.safe_terms = {\n",
    "            'geographical': [\n",
    "                'earth', 'world', 'planet', 'continent', 'country', 'nation', 'location', 'place', 'where', 'map'\n",
    "            ],\n",
    "            'educational': [\n",
    "                'learn', 'study', 'education', 'school', 'university', 'research', 'knowledge', 'information'\n",
    "            ],\n",
    "            'general': [\n",
    "                'help', 'question', 'answer', 'explain', 'describe', 'tell', 'what', 'how', 'why', 'when'\n",
    "            ]\n",
    "        }\n",
    "        self._initialize_system()  # Call system initialization method\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _initialize_system(self):\n",
    "        \"\"\"Initialize the entire matching system.\"\"\"\n",
    "        self._load_model()  # Load the transformer model\n",
    "        self._load_embeddings()  # Load pre-computed embeddings\n",
    "        self._setup_category_hierarchies()  # Setup category hierarchy mappings\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "            Load the sentence transformer model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Default local path for saved embedding model snapshots\n",
    "            base_path = \"./Embedding_Model/all_mini_embed_model/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/\"\n",
    "            \n",
    "            # List all directories inside base_path (each represents a commit hash)\n",
    "            hashes = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "            \n",
    "            # If no snapshot hash directories are found, raise an error\n",
    "            if not hashes:\n",
    "                raise ValueError(\"No snapshot hash folders found!\")\n",
    "            \n",
    "            # Pick the latest snapshot folder (last after sorting)\n",
    "            commit_hash = sorted(hashes)[-1]\n",
    "            \n",
    "            # Build the full path to the chosen snapshot\n",
    "            model_path = os.path.join(base_path, commit_hash)\n",
    "            \n",
    "            # Load the SentenceTransformer model from the local snapshot\n",
    "            self.model = SentenceTransformer(model_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Return exception if any error occurs\n",
    "            return e\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    def _load_embeddings(self):\n",
    "        \"\"\"Load and encode topics and sentences data.\"\"\"\n",
    "        try:\n",
    "            # Path to JSON file containing topics to avoid\n",
    "            topics_path = \"./Files/topics_to_avoid.json\"\n",
    "            \n",
    "            # Open and load topics JSON file\n",
    "            with open(topics_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                topics_data = json.load(f)\n",
    "            \n",
    "            # Path to JSON file containing sentences to avoid\n",
    "            sentences_path = \"./Files/sentences_to_avoid.json\"\n",
    "            \n",
    "            # Open and load sentences JSON file\n",
    "            with open(sentences_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                sentences_data = json.load(f)\n",
    "            \n",
    "            # Encode all words in topics into embeddings and store by category\n",
    "            for category, words in topics_data.items():\n",
    "                self.embeddings[category] = self.model.encode(words, convert_to_numpy=True)\n",
    "            \n",
    "            # Encode all sentences into embeddings and store by category\n",
    "            for category, sentences in sentences_data.items():\n",
    "                self.sentence_embeddings[category] = self.model.encode(sentences, convert_to_numpy=True)\n",
    "    \n",
    "        except Exception as e:\n",
    "            # Return the exception object if an error occurs\n",
    "            return e\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "    def _setup_category_hierarchies(self):\n",
    "        \"\"\"\n",
    "            Setup hierarchical category relationships for better organization.\n",
    "        \"\"\"\n",
    "        # Define dictionary of category hierarchies\n",
    "        self.category_hierarchies = {\n",
    "            # Security threats category with child categories and severity level\n",
    "            'security_threats': {\n",
    "                'children': ['violence_and_crime', 'weapons_and_warfare', 'terrorism', 'extremism_and_hate_groups'],\n",
    "                'severity': 'critical'\n",
    "            },\n",
    "            # Personal harm category with child categories and severity level\n",
    "            'personal_harm': {\n",
    "                'children': ['self_harm', 'toxicity', 'harassment'],\n",
    "                'severity': 'high'\n",
    "            },\n",
    "            # Illegal activities category with child categories and severity level\n",
    "            'illegal_activities': {\n",
    "                'children': ['drugs_and_substances', 'fraud_and_scams', 'financial_and_illegal_activities'],\n",
    "                'severity': 'high'\n",
    "            },\n",
    "            # Privacy violations category with child categories and severity level\n",
    "            'privacy_violations': {\n",
    "                'children': ['pii', 'cybersecurity_threats'],\n",
    "                'severity': 'medium'\n",
    "            },\n",
    "            # Inappropriate content category with child categories and severity level\n",
    "            'inappropriate_content': {\n",
    "                'children': ['sexual_content', 'child_safety'],\n",
    "                'severity': 'critical'\n",
    "            },\n",
    "            # Misinformation category with child categories and severity level\n",
    "            'misinformation': {\n",
    "                'children': ['misinformation_and_disinformation'],\n",
    "                'severity': 'high'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _clean_query(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "            Clean and tokenize query, removing stopwords.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load English stopwords into a set for faster lookup\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            \n",
    "            # Convert query to lowercase and tokenize into words\n",
    "            words = word_tokenize(query.lower())\n",
    "            \n",
    "            # Filter words: keep only alphabetic tokens, exclude stopwords, and require length > 2\n",
    "            filtered_words = [w for w in words if w.isalpha() and w not in stop_words and len(w) > 2]\n",
    "            \n",
    "            # Return the cleaned list of words\n",
    "            return filtered_words\n",
    "            \n",
    "        except Exception:\n",
    "            # Fallback: simple split and lowercase if tokenization/stopwords fail\n",
    "            return [w.lower() for w in query.split() if len(w) > 2]\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    def _is_safe_query(self, query: str) -> bool:\n",
    "        \"\"\"\n",
    "            Check if query appears to be a legitimate educational/geographical question.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert query to lowercase for consistent matching\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # List of common educational patterns\n",
    "        educational_patterns = [\n",
    "            'where is', 'what is', 'how does', 'explain', 'tell me about',\n",
    "            'how to learn', 'what does', 'can you explain', 'help me understand'\n",
    "        ]\n",
    "        \n",
    "        # List of geographical keywords\n",
    "        geographical_indicators = ['country', 'city', 'location', 'capital', 'continent', 'map']\n",
    "        \n",
    "        # Common safe query starters\n",
    "        safe_starters = ['where', 'what', 'how', 'when', 'why', 'can you', 'please explain']\n",
    "\n",
    "        # OTHER SAFETY STANDARDS CAN BE GIVEN HERE\n",
    "        \n",
    "        # Check if query contains any educational patterns\n",
    "        has_educational_pattern = any(pattern in query_lower for pattern in educational_patterns)\n",
    "        \n",
    "        # Check if query contains any geographical indicators\n",
    "        has_geographical_indicator = any(indicator in query_lower for indicator in geographical_indicators)\n",
    "        \n",
    "        # Check if query starts with a safe phrase\n",
    "        starts_safely = any(query_lower.startswith(starter) for starter in safe_starters)\n",
    "        \n",
    "        # Count overlap of query words with predefined safe terms\n",
    "        query_words = set(query_lower.split())\n",
    "        safe_word_count = 0\n",
    "        for category, terms in self.safe_terms.items():\n",
    "            safe_word_count += len(query_words.intersection(terms))\n",
    "        \n",
    "        # Query is safe if it matches educational patterns,\n",
    "        # OR contains geographical indicators,\n",
    "        # OR starts safely and has at least 2 safe words\n",
    "        return (has_educational_pattern or \n",
    "                has_geographical_indicator or \n",
    "                (starts_safely and safe_word_count >= 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _is_common_word_match(self, matched_content: str, query: str) -> bool:\n",
    "        \"\"\"\n",
    "            Check if the match is due to a common word that could be innocent.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Split query into lowercase words and store in a set\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        # Split matched content into lowercase words and store in a set\n",
    "        matched_words = set(matched_content.lower().split())\n",
    "        \n",
    "        # Initialize an empty set to hold all safe terms\n",
    "        all_safe_terms = set()\n",
    "        \n",
    "        # Collect all safe terms from different categories into one flat set\n",
    "        for terms in self.safe_terms.values():\n",
    "            all_safe_terms.update(terms)\n",
    "        \n",
    "        # Find words common to both query and matched content\n",
    "        common_words = query_words.intersection(matched_words)\n",
    "        \n",
    "        # Return True if common words exist AND all of them are safe terms\n",
    "        return len(common_words) > 0 and common_words.issubset(all_safe_terms)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def _filter_false_positives(self, results: List[SimilarityResult], query: str) -> List[SimilarityResult]:\n",
    "        \"\"\"\n",
    "            Filter out likely false positives based on query context.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the query is recognized as safe\n",
    "        if self._is_safe_query(query):\n",
    "            # Initialize an empty list for filtered results\n",
    "            filtered_results = []\n",
    "            \n",
    "            # Loop through each similarity result\n",
    "            for result in results:\n",
    "                # If the result belongs to a critical category → lower threshold\n",
    "                if result.category in self.critical_category_thresholds:\n",
    "                    min_threshold = 0.75\n",
    "                else:\n",
    "                    # For non-critical categories → stricter threshold\n",
    "                    min_threshold = 0.88\n",
    "                \n",
    "                # Keep result only if similarity score passes threshold\n",
    "                if result.similarity_score >= min_threshold:\n",
    "                    # For critical categories, keep directly\n",
    "                    # For non-critical, check it’s not just a common word match\n",
    "                    if result.category in self.critical_category_thresholds or not self._is_common_word_match(result.matched_content, query):\n",
    "                        filtered_results.append(result)\n",
    "            \n",
    "            # Return the filtered results for safe queries\n",
    "            return filtered_results\n",
    "        \n",
    "        # If query is not safe, return results unfiltered\n",
    "        return results\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def _calculate_detailed_similarity(self, query: str, threshold: float = 0.45) -> List[SimilarityResult]:\n",
    "        \"\"\"\n",
    "            Calculate detailed similarity scores for both words and sentences.\n",
    "        \"\"\"\n",
    "        \n",
    "        results = []  # Initialize list to store similarity results\n",
    "        \n",
    "        # ---------------- Word-level analysis ----------------\n",
    "        \n",
    "        # Clean query into filtered words (remove stopwords, etc.)\n",
    "        filtered_words = self._clean_query(query)\n",
    "        \n",
    "        # If valid filtered words exist\n",
    "        if filtered_words:\n",
    "            # Encode query words into embeddings\n",
    "            query_word_embeddings = self.model.encode(filtered_words, convert_to_numpy=True, show_progress_bar=False)\n",
    "            \n",
    "            # Compare query word embeddings with stored risky word embeddings\n",
    "            for category, word_embeddings in self.embeddings.items():\n",
    "                for q_emb in query_word_embeddings:\n",
    "                    # Compute cosine similarity between query word and risky words\n",
    "                    similarities = cosine_similarity([q_emb], word_embeddings)[0]\n",
    "                    \n",
    "                    # Take the maximum similarity score\n",
    "                    max_sim = np.max(similarities)\n",
    "                    \n",
    "                    # If max similarity is above threshold, consider it a match\n",
    "                    if max_sim >= threshold:\n",
    "                        # Index of the most similar risky word\n",
    "                        max_idx = np.argmax(similarities)\n",
    "                        \n",
    "                        # Retrieve actual risky word from JSON file\n",
    "                        try:\n",
    "                            topics_path = \"./Files/topics_to_avoid.json\"\n",
    "                            with open(topics_path, \"r\") as f:\n",
    "                                topics_data = json.load(f)\n",
    "                            matched_word = topics_data[category][max_idx]\n",
    "                        except:\n",
    "                            matched_word = \"unknown\"  # If lookup fails\n",
    "                        \n",
    "                        # Append result with details\n",
    "                        results.append(SimilarityResult(\n",
    "                            category=category,\n",
    "                            similarity_score=float(max_sim),\n",
    "                            matched_content=matched_word,\n",
    "                            content_type='word',\n",
    "                            risk_level=self._determine_risk_level(max_sim, category)\n",
    "                        ))\n",
    "        \n",
    "        # ---------------- Sentence-level analysis ----------------\n",
    "        \n",
    "        # Encode the entire query as a sentence embedding\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True, show_progress_bar=False)[0]\n",
    "        \n",
    "        # Compare query sentence embedding with stored risky sentence embeddings\n",
    "        for category, sentence_embeddings in self.sentence_embeddings.items():\n",
    "            # Compute cosine similarities\n",
    "            similarities = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "            \n",
    "            # Take maximum similarity score\n",
    "            max_sim = np.max(similarities)\n",
    "            \n",
    "            # If max similarity exceeds threshold\n",
    "            if max_sim >= threshold:\n",
    "                # Index of the most similar risky sentence\n",
    "                max_idx = np.argmax(similarities)\n",
    "                \n",
    "                # Retrieve actual risky sentence from JSON file\n",
    "                try:\n",
    "                    sentences_path = \"./Files/sentences_to_avoid.json\"\n",
    "                    with open(sentences_path, \"r\") as f:\n",
    "                        sentences_data = json.load(f)\n",
    "                    matched_sentence = sentences_data[category][max_idx]\n",
    "                except:\n",
    "                    matched_sentence = \"unknown\"  # If lookup fails\n",
    "                \n",
    "                # Append result with details\n",
    "                results.append(SimilarityResult(\n",
    "                    category=category,\n",
    "                    similarity_score=float(max_sim),\n",
    "                    matched_content=matched_sentence,\n",
    "                    content_type='sentence',\n",
    "                    risk_level=self._determine_risk_level(max_sim, category)\n",
    "                ))\n",
    "        \n",
    "        # ---------------- Post-processing ----------------\n",
    "        \n",
    "        # Sort results by similarity score in descending order\n",
    "        results.sort(key=lambda x: x.similarity_score, reverse=True)\n",
    "        \n",
    "        # Apply false positive filtering for safer results\n",
    "        filtered_results = self._filter_false_positives(results, query)\n",
    "        \n",
    "        # Return the final filtered list of similarity results\n",
    "        return filtered_results\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def _determine_risk_level(self, similarity_score: float, category: str = None) -> str:\n",
    "        \"\"\"Determine risk level based on similarity score and category-specific thresholds.\"\"\"\n",
    "        \n",
    "        # If category is critical, use its specific threshold\n",
    "        if category and category in self.critical_category_thresholds:\n",
    "            critical_threshold = self.critical_category_thresholds[category]\n",
    "            if similarity_score >= critical_threshold:\n",
    "                return 'high'  # Elevate critical category matches to high risk\n",
    "        \n",
    "        # Standard threshold logic for non-critical categories\n",
    "        if similarity_score >= self.risk_thresholds['high_risk']:\n",
    "            return 'high'  # High similarity → high risk\n",
    "        elif similarity_score >= self.risk_thresholds['medium_risk']:\n",
    "            return 'medium'  # Medium similarity → medium risk\n",
    "        elif similarity_score >= self.risk_thresholds['low_risk']:\n",
    "            return 'low'  # Low similarity → low risk\n",
    "        else:\n",
    "            return 'minimal'  # Below all thresholds → minimal risk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_category_severity(self, category: str) -> str:\n",
    "        \"\"\"Get severity level for a category based on hierarchies.\"\"\"\n",
    "        for parent, info in self.category_hierarchies.items():\n",
    "            if category in info['children']:\n",
    "                return info['severity']\n",
    "        return 'medium'  # default\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def analyze_query(self, query: str) -> QueryAnalysis:\n",
    "        \"\"\"\n",
    "        Perform comprehensive analysis of a user query.\n",
    "        Returns detailed analysis including categories, similarities, and risk assessment.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Record start time for processing time calculation\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Compute detailed similarity scores for the query using a balanced threshold\n",
    "        similarity_results = self._calculate_detailed_similarity(query, threshold=0.60)\n",
    "        \n",
    "        # Extract unique categories from similarity results that were flagged\n",
    "        flagged_categories = list(set([result.category for result in similarity_results]))\n",
    "        \n",
    "        # Identify high and medium risk matches\n",
    "        high_risk_matches = [r for r in similarity_results if r.risk_level in ['high', 'medium']]\n",
    "        \n",
    "        # Query is considered safe if no high/medium risk matches exist\n",
    "        is_safe = len(high_risk_matches) == 0\n",
    "        \n",
    "        # Get the highest similarity score among all matches\n",
    "        highest_similarity = similarity_results[0].similarity_score if similarity_results else 0.0\n",
    "        \n",
    "        # Determine the primary category with the highest similarity\n",
    "        primary_category = similarity_results[0].category if similarity_results else None\n",
    "        \n",
    "        # Generate a summarized risk assessment based on flagged categories and similarity results\n",
    "        risk_assessment = self._generate_risk_assessment(similarity_results, flagged_categories)\n",
    "        \n",
    "        # Calculate total processing time\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Return a structured QueryAnalysis object with all relevant information\n",
    "        return QueryAnalysis(\n",
    "            query=query,\n",
    "            is_safe=is_safe,\n",
    "            flagged_categories=flagged_categories,\n",
    "            detailed_matches=similarity_results[:10],  # Only include top 10 matches\n",
    "            highest_similarity=highest_similarity,\n",
    "            primary_category=primary_category,\n",
    "            risk_assessment=risk_assessment,\n",
    "            processing_time=processing_time\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _generate_risk_assessment(self, similarity_results: List[SimilarityResult], flagged_categories: List[str]) -> str:\n",
    "        \"\"\"Generate comprehensive risk assessment text.\"\"\"\n",
    "        \n",
    "        # If no similarity matches found, return safe\n",
    "        if not similarity_results:\n",
    "            return \"Safe - No concerning content detected\"\n",
    "        \n",
    "        # Separate high-risk matches\n",
    "        high_risk = [r for r in similarity_results if r.risk_level == 'high']\n",
    "        \n",
    "        # Separate medium-risk matches\n",
    "        medium_risk = [r for r in similarity_results if r.risk_level == 'medium']\n",
    "        \n",
    "        # If high-risk match exists, report it\n",
    "        if high_risk:\n",
    "            primary_cat = high_risk[0].category.replace('_', ' ').title()  # Format category for readability\n",
    "            return f\"High Risk - Detected {primary_cat} content (confidence: {high_risk[0].similarity_score:.2f})\"\n",
    "        \n",
    "        # If medium-risk match exists, report it\n",
    "        elif medium_risk:\n",
    "            primary_cat = medium_risk[0].category.replace('_', ' ').title()  # Format category\n",
    "            return f\"Medium Risk - Potential {primary_cat} content (confidence: {medium_risk[0].similarity_score:.2f})\"\n",
    "        \n",
    "        # Otherwise, report as low risk with minor flagged categories\n",
    "        else:\n",
    "            return f\"Low Risk - Minor concerns detected in {len(flagged_categories)} categories\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    def get_category_breakdown(self, query: str) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Get detailed breakdown by category with statistics.\"\"\"\n",
    "        \n",
    "        # Perform comprehensive analysis of the query\n",
    "        analysis = self.analyze_query(query)\n",
    "        \n",
    "        # Initialize category statistics dictionary with default structure\n",
    "        category_stats = defaultdict(lambda: {\n",
    "            'matches': [],            # List of similarity results\n",
    "            'max_similarity': 0.0,    # Maximum similarity score in this category\n",
    "            'avg_similarity': 0.0,    # Average similarity score\n",
    "            'risk_level': 'minimal',  # Highest risk level in this category\n",
    "            'severity': 'low'         # Severity level of category\n",
    "        })\n",
    "        \n",
    "        # Populate statistics per category based on detailed matches\n",
    "        for result in analysis.detailed_matches:\n",
    "            cat = result.category\n",
    "            \n",
    "            # Append match to category list\n",
    "            category_stats[cat]['matches'].append(result)\n",
    "            \n",
    "            # Update maximum similarity score\n",
    "            category_stats[cat]['max_similarity'] = max(\n",
    "                category_stats[cat]['max_similarity'], \n",
    "                result.similarity_score\n",
    "            )\n",
    "            \n",
    "            # Retrieve and assign category severity\n",
    "            category_stats[cat]['severity'] = self._get_category_severity(cat)\n",
    "            \n",
    "            # Update category risk level to the highest among matches\n",
    "            current_risk = category_stats[cat]['risk_level']\n",
    "            new_risk = result.risk_level\n",
    "            risk_order = ['minimal', 'low', 'medium', 'high']\n",
    "            if risk_order.index(new_risk) > risk_order.index(current_risk):\n",
    "                category_stats[cat]['risk_level'] = new_risk\n",
    "        \n",
    "        # Calculate average similarity for each category\n",
    "        for cat, stats in category_stats.items():\n",
    "            if stats['matches']:\n",
    "                stats['avg_similarity'] = np.mean([m.similarity_score for m in stats['matches']])\n",
    "        \n",
    "        # Convert defaultdict to regular dict and return\n",
    "        return dict(category_stats)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8d193-d449-496a-b061-c7b3125610af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc804765-7f7f-47d4-9504-d529ad2be32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_detailed_results(results: List[Dict[str, Any]]):\n",
    "    \"\"\"Print detailed analysis results in a formatted way.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE QUERY ANALYSIS RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n--- Query {i} ---\")\n",
    "        print(f\"Query: {result['query']}\")\n",
    "        print(f\"Status: {result.get('status', 'unknown').upper()}\")\n",
    "        \n",
    "        if 'analysis' in result:\n",
    "            analysis = result['analysis']\n",
    "            print(f\"Safety: {'SAFE' if analysis.is_safe else 'FLAGGED'}\")\n",
    "            print(f\"Risk Assessment: {analysis.risk_assessment}\")\n",
    "            \n",
    "            if analysis.flagged_categories:\n",
    "                print(f\"Flagged Categories: {', '.join(analysis.flagged_categories)}\")\n",
    "            \n",
    "            if analysis.detailed_matches:\n",
    "                print(\"Top Matches:\")\n",
    "                for match in analysis.detailed_matches[:3]:\n",
    "                    print(f\"  • {match.category}: {match.similarity_score:.3f} ({match.risk_level} risk)\")\n",
    "        \n",
    "        print(f\"Response: {result['response'][:200]}{'...' if len(result['response']) > 200 else ''}\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f496f-59d3-4131-ac28-e25222fab41d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print_detailed_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075e9fa-9185-4d81-9c33-d432059ca0ca",
   "metadata": {},
   "source": [
    "```PYTHON\n",
    "\"\"\"\n",
    "def process_query_batch(queries: List[str], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple queries in parallel with comprehensive analysis.\"\"\"\n",
    "    \n",
    "    def process_single_query(query):\n",
    "        return matcher.process_query_comprehensive(query)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_query, query) for query in queries]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing query: {e}\")\n",
    "                results.append({\n",
    "                    'query': 'unknown',\n",
    "                    'error': str(e),\n",
    "                    'status': 'error'\n",
    "                })\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    logger.info(f\"Processed {len(queries)} queries in {end_time - start_time:.2f} seconds\")\n",
    "    return results\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f79cb-c016-475f-b13a-54ab580decc9",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e84ab-1f1c-4695-8cb3-e26707ac8b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bfd6cd-8956-4939-b69c-d15ece7eb45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98a15e-45c8-49f3-9fc9-5369238312de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50f12a-f2b8-4702-9f66-d1509ef19248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fdacb-1b0f-4b7a-b0e1-9f81bc6036e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
